import torch.nn.functional as F
import torch.optim.lr_scheduler
from matplotlib import pyplot as plt
from torch import optim
from tqdm import tqdm

from segmentation.utils import DiceLoss, FocalLoss


def train_epoch(model, train_loader, criterion, optimizer, device, num_classes, grad_clip_value=None):
    model.train()
    total_loss = 0
    all_preds, all_labels = [], []
    dices, ious, accs = [], [], []

    progress_bar = tqdm(train_loader, desc="Training")
    for images, masks in train_loader:
        images, masks = images.to(device), masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)

        loss.backward()

        if grad_clip_value is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)

        optimizer.step()

        total_loss += loss.item()

        preds = outputs.argmax(dim=1) if num_classes > 1 else (torch.sigmoid(outputs) > 0.5).long()
        dice, iou, acc = _compute_metrics(outputs, masks, num_classes)

        dices.append(dice)
        ious.append(iou)
        accs.append(acc)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(masks.cpu().numpy())

        progress_bar.set_postfix({"Loss": loss.item(), "Dice": dice, "IoU": iou})

    avg_loss = total_loss / len(train_loader)
    return avg_loss, np.mean(dices), np.mean(ious), np.mean(accs), np.array(all_preds), np.array(all_labels)




def validate_epoch(model, val_loader, criterion, device, num_classes):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    dices, ious, accs = [], [], []

    with torch.no_grad():
        for images, masks in tqdm(val_loader, desc="Validating"):
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            loss = criterion(outputs, masks)
            total_loss += loss.item()

            preds = outputs.argmax(dim=1) if num_classes > 1 else (torch.sigmoid(outputs) > 0.5).long()
            dice, iou, acc = _compute_metrics(outputs, masks, num_classes)

            dices.append(dice)
            ious.append(iou)
            accs.append(acc)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(masks.cpu().numpy())

    avg_loss = total_loss / len(val_loader)
    return avg_loss, np.mean(dices), np.mean(ious), np.mean(accs), np.array(all_preds), np.array(all_labels)


import torch
import torch.nn as nn
import numpy as np


def train_model(model, train_loader, val_loader,
                num_epochs=50, lr=1e-4, weight_decay=0.01,
                loss_name="crossentropy", optimizer_name="adamw", scheduler_name="cosine",
                device='cuda', patience=10, min_delta=1e-4,
                grad_clip_value=None, display_interval=10, model_save_path='best_model.pth'):
    """
    åˆ†å‰²ä»»åŠ¡çš„è®­ç»ƒä¸»å‡½æ•°ï¼ˆç»“æž„ä¸Žåˆ†ç±»ç‰ˆæœ¬ç»Ÿä¸€ï¼‰

    Args:
        model (torch.nn.Module): åˆ†å‰²æ¨¡åž‹
        train_loader (DataLoader): è®­ç»ƒæ•°æ®
        val_loader (DataLoader): éªŒè¯æ•°æ®
        num_epochs (int): æœ€å¤§è®­ç»ƒè½®æ•°
        lr (float): å­¦ä¹ çŽ‡
        weight_decay (float): æƒé‡è¡°å‡
        loss_name (str): æŸå¤±å‡½æ•°
        optimizer_name (str): ä¼˜åŒ–å™¨
        scheduler_name (str): å­¦ä¹ çŽ‡è°ƒåº¦å™¨
        device (str): è®¾å¤‡
        patience (int): æ—©åœé˜ˆå€¼
        min_delta (float): Dice æå‡æœ€å°é˜ˆå€¼
        grad_clip_value (float): æ¢¯åº¦è£å‰ª
        display_interval (int): æŒ‡æ ‡å¹³å‡†æ‰“å°é—´éš”
        model_save_path (str): æœ€ä½³æ¨¡åž‹ä¿å­˜è·¯å¾„

    Returns:
        best_model_state (dict): æœ€ä¼˜æ¨¡åž‹ state_dict
        metrics (dict): è®­ç»ƒè¿‡ç¨‹çš„æŒ‡æ ‡
    """

    criterion, optimizer, scheduler = _build_training_config(
        model.parameters(), loss_name=loss_name,
        optimizer_name=optimizer_name,
        scheduler_name=scheduler_name,
        lr=lr, weight_decay=weight_decay,
        num_epochs=num_epochs, patience=patience
    )

    train_losses, val_losses = [], []
    train_dices, val_dices = [], []
    train_ious, val_ious = [], []
    train_accs, val_accs = [], []

    best_val_dice = 0
    best_model_state = None
    patience_counter = 0

    epoch_group_metrics = {
        'train_loss': [], 'val_loss': [],
        'train_dice': [], 'val_dice': [],
        'train_iou': [], 'val_iou': [],
        'train_acc': [], 'val_acc': []
    }

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 50)

        train_loss, train_dice, train_iou, train_acc, _, _ = train_epoch(
            model, train_loader, criterion, optimizer, device,
            grad_clip_value=grad_clip_value
        )

        val_loss, val_dice, val_iou, val_acc, _, _ = validate_epoch(
            model, val_loader, criterion, device
        )

        scheduler.step()

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_dices.append(train_dice)
        val_dices.append(val_dice)
        train_ious.append(train_iou)
        val_ious.append(val_iou)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        epoch_group_metrics['train_loss'].append(train_loss)
        epoch_group_metrics['val_loss'].append(val_loss)
        epoch_group_metrics['train_dice'].append(train_dice)
        epoch_group_metrics['val_dice'].append(val_dice)
        epoch_group_metrics['train_iou'].append(train_iou)
        epoch_group_metrics['val_iou'].append(val_iou)
        epoch_group_metrics['train_acc'].append(train_acc)
        epoch_group_metrics['val_acc'].append(val_acc)

        # --- æ‰“å° ---
        print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        print(f"Train Dice: {train_dice:.4f}, Val Dice: {val_dice:.4f}")
        print(f"Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}")
        print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
        print(f"Learning Rate: {scheduler.get_last_lr()[0]:.6f}")

        # --- å¹³å‡æŒ‡æ ‡æ‰“å° ---
        if (epoch + 1) % display_interval == 0:
            print("\n" + "=" * 60)
            print(f"AVERAGE METRICS FOR EPOCHS {epoch - display_interval + 2}-{epoch + 1}")
            print("=" * 60)
            print(f"Avg Train Loss: {np.mean(epoch_group_metrics['train_loss']):.4f} "
                  f"| Avg Val Loss: {np.mean(epoch_group_metrics['val_loss']):.4f}")
            print(f"Avg Train Dice: {np.mean(epoch_group_metrics['train_dice']):.4f} "
                  f"| Avg Val Dice: {np.mean(epoch_group_metrics['val_dice']):.4f}")
            print(f"Avg Train IoU: {np.mean(epoch_group_metrics['train_iou']):.4f} "
                  f"| Avg Val IoU: {np.mean(epoch_group_metrics['val_iou']):.4f}")
            print(f"Avg Train Acc: {np.mean(epoch_group_metrics['train_acc']):.4f} "
                  f"| Avg Val Acc: {np.mean(epoch_group_metrics['val_acc']):.4f}")
            print("=" * 60)

            # æ¸…ç©ºåˆ†ç»„æŒ‡æ ‡
            for key in epoch_group_metrics:
                epoch_group_metrics[key] = []

        if val_dice > best_val_dice + min_delta:
            best_val_dice = val_dice
            best_model_state = model.state_dict().copy()
            patience_counter = 0

            torch.save({
                'epoch': epoch,
                'model_state_dict': best_model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_dice': best_val_dice,
                'val_iou': val_iou,
                'val_acc': val_acc,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_dices': train_dices,
                'val_dices': val_dices,
                'train_ious': train_ious,
                'val_ious': val_ious,
                'train_accs': train_accs,
                'val_accs': val_accs
            }, model_save_path)
            print(f"ðŸŽ‰ New best model saved with Dice: {best_val_dice:.4f}")
        else:
            patience_counter += 1
            print(f"â³ No improvement for {patience_counter} epochs")

        if patience_counter >= patience:
            print(f"\nðŸ›‘ Early stopping triggered after {patience} epochs without improvement")
            break

    print("\n" + "=" * 60)
    print("TRAINING COMPLETED!")
    print("=" * 60)
    print(f"Best Validation Dice: {best_val_dice:.4f}")
    print(f"Total Epochs: {epoch + 1}")

    _plot_segmentation_results(train_losses, val_losses,  # è¿™é‡Œè¦æ”¹æˆç”» Dice/IoU/Acc æ›²çº¿çš„å‡½æ•°
                               train_dices, val_dices,
                               train_ious, val_ious,
                               train_accs, val_accs)

    return best_model_state, {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_dices': train_dices,
        'val_dices': val_dices,
        'train_ious': train_ious,
        'val_ious': val_ious,
        'train_accs': train_accs,
        'val_accs': val_accs,
        'best_val_dice': best_val_dice
    }


def _build_training_config(model_params, loss_name="crossentropy", optimizer_name="adamw",
                           scheduler_name=None, lr=1e-5, weight_decay=1e-4, num_epochs=50, patience=5):
    """
    æž„å»ºè®­ç»ƒé…ç½®ï¼šæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨ã€‚

    Args:
        model_params: æ¨¡åž‹å‚æ•°ï¼Œä¾›ä¼˜åŒ–å™¨ä½¿ç”¨ã€‚
        loss_name: æŸå¤±å‡½æ•°ç±»åž‹
        optimizer_name: ä¼˜åŒ–å™¨ç±»åž‹
        scheduler_name: å­¦ä¹ çŽ‡è°ƒåº¦å™¨ç±»åž‹
        lr (float, optional): å­¦ä¹ çŽ‡ï¼Œé»˜è®¤ 1e-5ã€‚
        weight_decay (float, optional): æƒé‡è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ 1e-4ã€‚
        num_epochs (int, optional): æ€»è®­ç»ƒè½®æ•°ï¼Œç”¨äºŽéƒ¨åˆ†è°ƒåº¦å™¨ï¼Œé»˜è®¤ 50ã€‚
        patience (int, optional): éªŒè¯é›†åœæ»žæ—¶é™ä½Žå­¦ä¹ çŽ‡çš„ç­‰å¾…è½®æ•°ï¼Œä»…å¯¹ ReduceLROnPlateau æœ‰æ•ˆï¼Œé»˜è®¤ 5ã€‚

    Returns:
        tuple:
            loss_fn (torch.nn.Module): é€‰å®šçš„æŸå¤±å‡½æ•°å®žä¾‹ã€‚
            optimizer (torch.optim.Optimizer): é€‰å®šçš„ä¼˜åŒ–å™¨å®žä¾‹ã€‚
            scheduler (torch.optim.lr_scheduler._LRScheduler or None): é€‰å®šçš„å­¦ä¹ çŽ‡è°ƒåº¦å™¨å®žä¾‹ï¼Œè‹¥ scheduler_name ä¸º None åˆ™è¿”å›ž Noneã€‚
    """

    # æŸå¤±å‡½æ•°
    loss_map = {
        # ðŸŽ¯ å¤šåˆ†ç±»åˆ†å‰²æ ‡å‡†
        "crossentropy": nn.CrossEntropyLoss(),
        # âœ… äºŒåˆ†ç±» / å¤šæ ‡ç­¾
        "bce": nn.BCEWithLogitsLoss(),
        # ðŸ§© Dice æŸå¤±ï¼ˆéœ€è‡ªå®šä¹‰å®žçŽ°ï¼‰
        "dice": DiceLoss(),
        # ðŸ”— Dice + CE
        "dice_ce": lambda: DiceLoss() + nn.CrossEntropyLoss(),
        # ðŸ”— Dice + BCE
        "dice_bce": lambda: DiceLoss() + nn.BCEWithLogitsLoss(),
        # ðŸŽ¯ ç±»åˆ«ä¸å¹³è¡¡æ—¶å¸¸ç”¨
        "focal": FocalLoss(),
    }
    loss_fn = loss_map[loss_name]

    # ä¼˜åŒ–å™¨
    opt_map = {
        # âš™ï¸ Transformer / åˆ†å‰²å¸¸ç”¨
        "adamw": optim.AdamW(model_params, lr=lr, weight_decay=weight_decay),
        # âš¡ å¿«é€Ÿæ”¶æ•›
        "adam": optim.Adam(model_params, lr=lr, weight_decay=weight_decay),
        # ðŸƒ ç¨³å®šæ”¶æ•›ï¼Œå¤§æ•°æ®é›†å¸¸ç”¨
        "sgd": optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=True),
        # ðŸŒŠ UNet åŽŸç‰ˆå¸¸ç”¨
        "rmsprop": optim.RMSprop(model_params, lr=lr, alpha=0.9, weight_decay=weight_decay),
    }
    optimizer = opt_map[optimizer_name]

    # å­¦ä¹ çŽ‡è°ƒåº¦å™¨
    sched_map = {
        # ðŸŒŠ ä½™å¼¦é€€ç«
        "cosine": optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs),
        # ðŸ”¥ðŸŒŠ ä½™å¼¦ + çƒ­é‡å¯
        "cosine_warmup": optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2),
        # ðŸ›‘ éªŒè¯é›†åœæ»žæ—¶é™ä½Ž lr
        "reduce_on_plateau": optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience),
        # ðŸ“‰ PolyLRï¼ˆåˆ†å‰²è®ºæ–‡å¸¸è§ï¼‰
        "poly": torch.optim.lr_scheduler.LambdaLR(
            optimizer, lr_lambda=lambda epoch: (1 - epoch / num_epochs) ** 0.9)
    }
    scheduler = sched_map[scheduler_name] if scheduler_name else None

    return loss_fn, optimizer, scheduler


def _plot_segmentation_results(train_losses, val_losses,
                               train_dices, val_dices,
                               train_ious, val_ious,
                               train_accs, val_accs,
                               save_path='segmentation_results.png'):
    """
    ç»˜åˆ¶åˆ†å‰²ä»»åŠ¡è®­ç»ƒè¿‡ç¨‹çš„æ›²çº¿ (Loss, Dice, IoU, Acc)

    Args:
        train_losses (list[float]): è®­ç»ƒ Loss
        val_losses (list[float]): éªŒè¯ Loss
        train_dices (list[float]): è®­ç»ƒ Dice
        val_dices (list[float]): éªŒè¯ Dice
        train_ious (list[float]): è®­ç»ƒ IoU
        val_ious (list[float]): éªŒè¯ IoU
        train_accs (list[float]): è®­ç»ƒ Acc
        val_accs (list[float]): éªŒè¯ Acc
        save_path (str, optional): ä¿å­˜è·¯å¾„
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # ---- Loss ----
    axes[0, 0].plot(train_losses, label='Train Loss', color='blue')
    axes[0, 0].plot(val_losses, label='Val Loss', color='orange')
    axes[0, 0].set_title('Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()

    # ---- Dice ----
    axes[0, 1].plot(train_dices, label='Train Dice', color='blue')
    axes[0, 1].plot(val_dices, label='Val Dice', color='orange')
    axes[0, 1].set_title('Dice Coefficient')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Dice')
    axes[0, 1].legend()

    # ---- IoU ----
    axes[1, 0].plot(train_ious, label='Train IoU', color='blue')
    axes[1, 0].plot(val_ious, label='Val IoU', color='orange')
    axes[1, 0].set_title('Intersection over Union (IoU)')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('IoU')
    axes[1, 0].legend()

    # ---- Accuracy ----
    axes[1, 1].plot(train_accs, label='Train Accuracy', color='blue')
    axes[1, 1].plot(val_accs, label='Val Accuracy', color='orange')
    axes[1, 1].set_title('Pixel Accuracy')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.savefig(save_path)
    plt.show()


def _compute_metrics(pred, target, num_classes):
    """
    è®¡ç®—åˆ†å‰²ä»»åŠ¡çš„ Dice, IoU, Acc
    Args:
        pred (torch.Tensor): æ¨¡åž‹è¾“å‡ºï¼Œå½¢çŠ¶ [B, C, H, W] (logits)
        target (torch.Tensor): çœŸå®žæ ‡ç­¾ï¼Œå½¢çŠ¶ [B, H, W] (int)
        num_classes (int): ç±»åˆ«æ•°
    Returns:
        dice (float), iou (float), acc (float)
    """
    if num_classes > 1:  # å¤šåˆ†ç±»
        preds = pred.argmax(dim=1)  # [B, H, W]
        pred_onehot = F.one_hot(preds, num_classes).permute(0, 3, 1, 2)  # [B, C, H, W]
        target_onehot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).to(pred.device)
    else:  # äºŒåˆ†ç±»
        preds = (torch.sigmoid(pred) > 0.5).long()  # [B, 1, H, W]
        pred_onehot = preds
        target_onehot = target.unsqueeze(1).long()

    # è½¬ float æ–¹ä¾¿è®¡ç®—
    pred_onehot = pred_onehot.float()
    target_onehot = target_onehot.float()

    # Dice & IoU
    intersection = (pred_onehot * target_onehot).sum(dim=(0, 2, 3))  # æŒ‰ç±»åˆ«ç®—
    union = pred_onehot.sum(dim=(0, 2, 3)) + target_onehot.sum(dim=(0, 2, 3))

    dice = ((2 * intersection + 1e-6) / (union + 1e-6)).mean().item()
    iou = ((intersection + 1e-6) / (union - intersection + 1e-6)).mean().item()

    # Accuracy
    acc = (preds == target).float().mean().item()

    return dice, iou, acc