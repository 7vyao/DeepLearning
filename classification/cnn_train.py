import copy

import numpy as np
import torch.nn.utils
from matplotlib import pyplot as plt
from sklearn.metrics import f1_score, classification_report
from torch import nn, optim
from tqdm import tqdm


def train_epoch(model, train_loader, criterion, optimizer, device, grad_clip_value=None):
    """
    å¯¹æ¨¡å‹æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰ã€‚

    Args:
        model (torch.nn.Module): è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_loader (torch.utils.data.DataLoader): åŒ…å«è®­ç»ƒæ•°æ®çš„ DataLoaderï¼Œæ¯ä¸ªæ‰¹æ¬¡è¿”å› (images, labels)ã€‚
        criterion (torch.nn.Module): æŸå¤±å‡½æ•°ã€‚
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨ã€‚
        device (torch.device or str): æ¨¡å‹å’Œæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚
        grad_clip_value (float, optional): æ¢¯åº¦å‰ªè£é˜ˆå€¼ï¼Œè¶…è¿‡è¯¥å€¼çš„æ¢¯åº¦å°†è¢«è£å‰ªã€‚é»˜è®¤ None è¡¨ç¤ºä¸è£å‰ªã€‚

    Returns:
        avg_loss (float): è¯¥è½®è®­ç»ƒçš„å¹³å‡æŸå¤±ã€‚
        accuracy (float): è¯¥è½®è®­ç»ƒçš„æ•´ä½“å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€‚
        f1 (float): è¯¥è½®è®­ç»ƒçš„å®å¹³å‡ F1 åˆ†æ•°ã€‚
        all_preds (np.ndarray): æœ¬è½®è®­ç»ƒæ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ã€‚
        all_labels (np.ndarray): æœ¬è½®è®­ç»ƒæ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾ã€‚
    """

    model.train()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(train_loader, desc='Training')
    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()                           # æ¸…ç©ºä¸Šæ¬¡è¿­ä»£çš„æ¢¯åº¦
        outputs = model(images)                         # å‰å‘ä¼ æ’­ï¼Œè·å¾—é¢„æµ‹å€¼
        loss = criterion(outputs, labels)               # è®¡ç®—æŸå¤±

        loss.backward()                                 # åå‘ä¼ æ’­

        if grad_clip_value is not None:                 # æ¢¯åº¦å‰ªè£
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)

        optimizer.step()                                # æ›´æ–°æ¨¡å‹å‚æ•°

        total_loss += loss.item()                       # è®¡ç®—æ€»æŸå¤±

        _, predicted = torch.max(outputs.data, 1)       # è·å–é¢„æµ‹ç±»åˆ«
        total += labels.size(0)
        correct += (predicted == labels).sum().item()   # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®æ•°é‡

        all_preds.extend(predicted.cpu().numpy())       # æ”¶é›†å•æ‰¹æ¬¡æ‰€æœ‰çš„é¢„æµ‹ç±»åˆ«
        all_labels.extend(labels.cpu().numpy())         # æ”¶é›†å•æ‰¹æ¬¡æ‰€æœ‰çš„çœŸå®ç±»åˆ«

        progress_bar.set_postfix({'Loss': loss.item(),
                                  'Acc': 100. * correct / total})

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    f1 = f1_score(all_labels, all_preds,
                  average='macro', zero_division=0)

    return avg_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)


def validate_epoch(model, val_loader, criterion, device):
    """
    å¯¹æ¨¡å‹æ‰§è¡Œä¸€ä¸ªéªŒè¯è½®æ¬¡ï¼ˆepochï¼‰ï¼Œè®¡ç®—æŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°ã€‚

    Args:
        model (torch.nn.Module): è¦éªŒè¯çš„æ¨¡å‹ã€‚
        val_loader (torch.utils.data.DataLoader): éªŒè¯æ•°æ®çš„ DataLoaderï¼Œæ¯ä¸ªæ‰¹æ¬¡è¿”å› (images, labels)ã€‚
        criterion (torch.nn.Module): æŸå¤±å‡½æ•°ã€‚
        device (torch.device or str): æ¨¡å‹å’Œæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚

    Returns:
        avg_loss (float): è¯¥è½®éªŒè¯çš„å¹³å‡æŸå¤±ã€‚
        accuracy (float): è¯¥è½®éªŒè¯çš„æ•´ä½“å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€‚
        f1 (float): è¯¥è½®éªŒè¯çš„å®å¹³å‡ F1 åˆ†æ•°ã€‚
        all_preds (np.ndarray): æœ¬è½®éªŒè¯æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ã€‚
        all_labels (np.ndarray): æœ¬è½®éªŒè¯æ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾ã€‚
    """

    model.eval()                                        # è¯„ä¼°æ¨¡å¼ï¼Œå›ºå®šæ¨¡å‹å‚æ•°ï¼Œç¦æ­¢åå‘ä¼ æ’­
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():                               # withoutæ¢¯åº¦ä¿¡æ¯
        for images, labels in tqdm(val_loader, desc="Validating"):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(val_loader)             # å¹³å‡æŸå¤±ï¼šæ€»æŸå¤± / batchæ•°é‡
    accuracy = 100. * correct / total                   # å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•° * 100%
    f1 = f1_score(all_labels, all_preds,                # ä½¿ç”¨æ‰€æœ‰é¢„æµ‹ç±»åˆ«å’ŒçœŸå®ç±»åˆ«ï¼Œè°ƒç”¨sklearn çš„å‡½æ•°è®¡ç®— F1
                  average='macro', zero_division=0)

    return avg_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)


class Trainer:
    """
    é€šç”¨æ¨¡å‹è®­ç»ƒå™¨ï¼Œå°è£…è®­ç»ƒã€éªŒè¯ã€æŒ‡æ ‡è®°å½•ã€æ—©åœã€æœ€ä½³æ¨¡å‹ä¿å­˜å’Œå¯è§†åŒ–åŠŸèƒ½ã€‚

    Args:
        Args:
        model (torch.nn.Module): éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_loader (DataLoader): è®­ç»ƒæ•°æ®çš„ DataLoaderã€‚
        val_loader (DataLoader): éªŒè¯æ•°æ®çš„ DataLoaderã€‚
        num_epochs (int, optional): æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ 50ã€‚
        lr (float, optional): å­¦ä¹ ç‡ï¼Œé»˜è®¤ 1e-4ã€‚
        weight_decay (float, optional): æƒé‡è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ 0.01ã€‚
        loss_name (str, optional): æŸå¤±å‡½æ•°åç§°ï¼Œé»˜è®¤ "crossentropy"ã€‚
        optimizer_name (str, optional): ä¼˜åŒ–å™¨åç§°ï¼Œé»˜è®¤ "adamw"ã€‚
        scheduler_name (str, optional): å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°ï¼Œé»˜è®¤ "cosine"ã€‚
        device (str or torch.device, optional): è®­ç»ƒè®¾å¤‡ï¼Œé»˜è®¤ "cuda"ã€‚
        patience (int, optional): æ—©åœè½®æ•°é˜ˆå€¼ï¼Œé»˜è®¤ 10ã€‚
        min_delta (float, optional): ç²¾åº¦æå‡é˜ˆå€¼ï¼Œç”¨äºæ—©åœåˆ¤å®šï¼Œé»˜è®¤ 1e-4ã€‚
        grad_clip_value (float, optional): æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ŒNone è¡¨ç¤ºä¸è£å‰ªï¼Œé»˜è®¤ Noneã€‚
        display_interval (int, optional): æ‰“å°å¹³å‡æŒ‡æ ‡çš„é—´éš”è½®æ•°ï¼Œé»˜è®¤ 10ã€‚
        model_save_path (str, optional): ä¿å­˜æœ€ä½³æ¨¡å‹çš„è·¯å¾„ï¼Œé»˜è®¤ 'best_model'ã€‚
        class_names (list[str], optional): åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«åç§°åˆ—è¡¨ï¼Œé»˜è®¤ Noneã€‚

    Attributes:
        model (torch.nn.Module): éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_loader (DataLoader): è®­ç»ƒæ•°æ®çš„ DataLoaderã€‚
        val_loader (DataLoader): éªŒè¯æ•°æ®çš„ DataLoaderã€‚
        num_epochs (int): æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
        device (torch.device): æ¨¡å‹è®­ç»ƒè®¾å¤‡ã€‚
        patience (int): æ—©åœè½®æ•°é˜ˆå€¼ã€‚
        min_delta (float): éªŒè¯æŒ‡æ ‡æå‡æœ€å°é˜ˆå€¼ï¼Œç”¨äºæ—©åœåˆ¤å®šã€‚
        grad_clip_value (float or None): æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ŒNone è¡¨ç¤ºä¸è£å‰ªã€‚
        display_interval (int): æ‰“å°å¹³å‡æŒ‡æ ‡çš„é—´éš”è½®æ•°ã€‚
        model_save_path (str): ä¿å­˜æœ€ä½³æ¨¡å‹çš„è·¯å¾„ã€‚
        class_names (list[str] or None): åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«åç§°åˆ—è¡¨ã€‚
        criterion (torch.nn.Module): æŸå¤±å‡½æ•°ã€‚
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨ã€‚
        scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
        history (dict): è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æŒ‡æ ‡å†å²ï¼ŒåŒ…æ‹¬æŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°ã€‚
        best_model_state (dict or None): éªŒè¯é›†ä¸Šæ€§èƒ½æœ€ä¼˜çš„æ¨¡å‹æƒé‡ã€‚

    Example:
        Example:
        >>>
        >>> train_dataset = MyDataset(train=True)
        >>> val_dataset = MyDataset(train=False)
        >>> train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        >>> val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        >>>
        >>> model = MyModel(num_classes=5)
        >>> trainer = Trainer(
        ...     model=model,
        ...     train_loader=train_loader,
        ...     val_loader=val_loader,
        ...     num_epochs=50,
        ...     lr=1e-4,
        ...     patience=10,
        ...     class_names=['ç±»A', 'ç±»B', 'ç±»C', 'ç±»D', 'ç±»E']
        ... )
        >>> best_model_state, history = trainer.train()
        >>> history['train_losses'][-1]
        0.1234
        >>> history['val_accuracies'][-1]
        92.56
    """

    def __init__(self, model, train_loader, val_loader,
                 num_epochs=50, lr=1e-4, weight_decay=0.01,
                 loss_name="crossentropy", optimizer_name="adamw", scheduler_name="cosine",
                 device='cuda', patience=10, min_delta=1e-4,
                 grad_clip_value=None, display_interval=10, model_save_path='best_model', class_names=None):

        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.num_epochs = num_epochs
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.patience = patience
        self.min_delta = min_delta
        self.grad_clip_value = grad_clip_value
        self.display_interval = display_interval
        self.model_save_path = model_save_path
        self.class_names = class_names

        self.criterion, self.optimizer, self.scheduler = _build_training_config(
            model.parameters(), loss_name=loss_name,
            optimizer_name=optimizer_name, scheduler_name=scheduler_name,
            lr=lr, weight_decay=weight_decay, num_epochs=num_epochs, patience=patience
        )

        self.history = {
            'train_losses': [], 'val_losses': [],
            'train_accuracies': [], 'val_accuracies': [],
            'train_f1s': [], 'val_f1s': []
        }
        self.best_val_acc = 0
        self.best_model_state = None

    def _plot_training_results(self, save_path='training_results.png'):

        train_losses, val_losses = self.history['train_losses'], self.history['val_losses']
        train_accuracies, val_accuracies = self.history['train_accuracies'], self.history['val_accuracies']
        train_f1s, val_f1s = self.history['train_f1s'], self.history['val_f1s']

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(train_losses, label='Train Loss', color='blue')
        axes[0, 0].plot(val_losses, label='Val Loss', color='red')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(train_accuracies, label='Train Accuracy', color='blue')
        axes[0, 1].plot(val_accuracies, label='Val Accuracy', color='red')
        axes[0, 1].set_title('Training and Validation Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # F1åˆ†æ•°æ›²çº¿
        axes[1, 0].plot(train_f1s, label='Train Macro F1', color='blue')
        axes[1, 0].plot(val_f1s, label='Val Macro F1', color='red')
        axes[1, 0].set_title('Training and Validation Macro F1')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Macro F1 Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # æœ€ååä¸ªepochçš„è¯¦ç»†æŒ‡æ ‡
        recent_epochs = min(10, len(train_losses))
        epochs_range = range(len(train_losses) - recent_epochs, len(train_losses))

        axes[1, 1].plot(epochs_range, train_accuracies[-recent_epochs:], 'o-', label='Train Acc', color='blue')
        axes[1, 1].plot(epochs_range, val_accuracies[-recent_epochs:], 'o-', label='Val Acc', color='red')
        axes[1, 1].set_title('Last 10 Epochs - Accuracy')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Accuracy (%)')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def train(self):

        model = self.model.to(self.device)
        criterion, optimizer, scheduler = self.criterion, self.optimizer, self.scheduler

        train_losses, val_losses = [], []
        train_accuracies, val_accuracies = [], []
        train_f1s, val_f1s = [], []

        best_val_acc = 0
        best_model_state = None
        patience_counter = 0

        epoch_group_metrics = {
            'train_loss': [], 'val_loss': [],
            'train_acc': [], 'val_acc': [],
            'train_f1': [], 'val_f1': []
        }

        for epoch in range(self.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.num_epochs}")
            print("-" * 50)

            train_loss, train_acc, train_f1, train_preds, train_labels = train_epoch(
                model, self.train_loader, criterion, optimizer, self.device, self.grad_clip_value
            )

            val_loss, val_acc, val_f1, val_preds, val_labels = validate_epoch(
                model, self.val_loader, criterion, self.device
            )

            scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

            train_losses.append(train_loss)  # ä¿å­˜æŒ‡æ ‡ç»“æœ
            val_losses.append(val_loss)
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)
            train_f1s.append(train_f1)
            val_f1s.append(val_f1)

            epoch_group_metrics['train_loss'].append(train_loss)
            epoch_group_metrics['val_loss'].append(val_loss)
            epoch_group_metrics['train_acc'].append(train_acc)
            epoch_group_metrics['val_acc'].append(val_acc)
            epoch_group_metrics['train_f1'].append(train_f1)
            epoch_group_metrics['val_f1'].append(val_f1)

            print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            print(f"Train Accuracy: {train_acc:.2f}%, Val Accuracy: {val_acc:.2f}%")
            print(f"Train Macro F1: {train_f1:.4f}, Val Macro F1: {val_f1:.4f}")
            print(f"Learning Rate: {scheduler.get_last_lr()[0]:.6f}")

            if (epoch + 1) % self.display_interval == 0:  # æ¯display_intervalä¸ªepochæ‰“å°ä¸€æ¬¡å¹³å‡†æŒ‡æ ‡
                print("\n" + "=" * 60)
                print(f"AVERAGE METRICS FOR EPOCHS {epoch - self.display_interval + 2}-{epoch + 1}")
                print("=" * 60)
                avg_train_loss = np.mean(epoch_group_metrics['train_loss'])
                avg_val_loss = np.mean(epoch_group_metrics['val_loss'])
                avg_train_acc = np.mean(epoch_group_metrics['train_acc'])
                avg_val_acc = np.mean(epoch_group_metrics['val_acc'])
                avg_train_f1 = np.mean(epoch_group_metrics['train_f1'])
                avg_val_f1 = np.mean(epoch_group_metrics['val_f1'])

                print(f"Average Train Loss: {avg_train_loss:.4f} | Average Val Loss: {avg_val_loss:.4f}")
                print(f"Average Train Accuracy: {avg_train_acc:.2f}% | Average Val Accuracy: {avg_val_acc:.2f}%")
                print(f"Average Train Macro F1: {avg_train_f1:.4f} | Average Val Macro F1: {avg_val_f1:.4f}")
                print("=" * 60)

                for key in epoch_group_metrics:  # æ¸…ç©ºç´¯ç§¯æŒ‡æ ‡
                    epoch_group_metrics[key] = []

            if val_acc > best_val_acc + self.min_delta:  # ä¿å­˜æœ€å¥½çš„æ¨¡å‹æƒé‡
                best_val_acc = val_acc
                best_model_state = copy.deepcopy(model.state_dict())
                patience_counter = 0

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': best_model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'val_accuracy': best_val_acc,
                    'val_f1': val_f1,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'train_accuracies': train_accuracies,
                    'val_accuracies': val_accuracies,
                    'train_f1s': train_f1s,
                    'val_f1s': val_f1s
                }, self.model_save_path)
                print(f"ğŸ‰ New best model saved with accuracy: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
                print(f"â³ No improvement for {patience_counter} epochs")

            if patience_counter >= self.patience:  # æ—©åœæœºåˆ¶
                print(f"\nğŸ›‘ Early stopping triggered after {self.patience} epochs without improvement")
                break

        print("\n" + "=" * 60)
        print("TRAINING COMPLETED!")
        print("=" * 60)
        print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
        print(f"Total Epochs: {epoch + 1}")

        # æ›´æ–°å†å²æŒ‡æ ‡
        self.history['train_losses'] = train_losses
        self.history['val_losses'] = val_losses
        self.history['train_accuracies'] = train_accuracies
        self.history['val_accuracies'] = val_accuracies
        self.history['train_f1s'] = train_f1s
        self.history['val_f1s'] = val_f1s

        self._plot_training_results()

        if best_model_state is not None:  # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆéªŒè¯
            model.load_state_dict(best_model_state)
            model.eval()

            final_val_loss, final_val_acc, final_val_f1, final_val_preds, final_val_labels = validate_epoch(
                model, self.val_loader, criterion, self.device
            )

            print("\nFinal Classification Report:")
            print("=" * 50)
            print(classification_report(final_val_labels, final_val_preds,
                                        target_names=self.class_names, digits=4))

        return best_model_state, {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'train_f1s': train_f1s,
            'val_f1s': val_f1s,
            'best_val_accuracy': best_val_acc
        }



def _build_training_config(model_params, loss_name="crossentropy", optimizer_name="adamw",
                           scheduler_name=None, lr=1e-5, weight_decay=1e-4, num_epochs=50, patience=5):
    """
    æ„å»ºè®­ç»ƒé…ç½®ï¼šæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    Args:
        model_params: æ¨¡å‹å‚æ•°ï¼Œä¾›ä¼˜åŒ–å™¨ä½¿ç”¨ã€‚
        loss_name: æŸå¤±å‡½æ•°ç±»å‹
        optimizer_name: ä¼˜åŒ–å™¨ç±»å‹
        scheduler_name: å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
        lr (float, optional): å­¦ä¹ ç‡ï¼Œé»˜è®¤ 1e-5ã€‚
        weight_decay (float, optional): æƒé‡è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ 1e-4ã€‚
        num_epochs (int, optional): æ€»è®­ç»ƒè½®æ•°ï¼Œç”¨äºéƒ¨åˆ†è°ƒåº¦å™¨ï¼Œé»˜è®¤ 50ã€‚
        patience (int, optional): éªŒè¯é›†åœæ»æ—¶é™ä½å­¦ä¹ ç‡çš„ç­‰å¾…è½®æ•°ï¼Œä»…å¯¹ ReduceLROnPlateau æœ‰æ•ˆï¼Œé»˜è®¤ 5ã€‚

    Returns:
        tuple:
            loss_fn (torch.nn.Module): é€‰å®šçš„æŸå¤±å‡½æ•°å®ä¾‹ã€‚
            optimizer (torch.optim.Optimizer): é€‰å®šçš„ä¼˜åŒ–å™¨å®ä¾‹ã€‚
            scheduler (torch.optim.lr_scheduler._LRScheduler or None): é€‰å®šçš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹ï¼Œè‹¥ scheduler_name ä¸º None åˆ™è¿”å› Noneã€‚
    """

    # æŸå¤±å‡½æ•°
    loss_map = {
        # ğŸ¯ æ ‡é…å¤šåˆ†ç±»æŸå¤±
        "crossentropy": nn.CrossEntropyLoss(),
        # ğŸ“‰ è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆlog_softmax æ­é…ï¼‰
        "nll": nn.NLLLoss(),
        # ğŸ§Š æ ‡ç­¾å¹³æ»‘ï¼Œé˜²è¿‡æ‹Ÿåˆ
        "label_smoothing": nn.CrossEntropyLoss(label_smoothing=0.1),
        # âœ… äºŒåˆ†ç±»ç¨³å®šé¦–é€‰
        "bce": nn.BCEWithLogitsLoss(),
    }
    loss_fn = loss_map[loss_name]

    # ä¼˜åŒ–å™¨
    opt_map = {
        # ğŸƒâ€â™‚ï¸ åŸºç¡€ç‰ˆï¼Œæ…¢ä½†ç¨³
        "sgd": optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay),
        # ğŸ¹ SGDåŠ é€Ÿç‰ˆ
        "sgd_nesterov": optim.SGD(model_params, lr=lr, momentum=0.9, nesterov=True, weight_decay=weight_decay),
        # âš¡ å¸¸è§è‡ªé€‚åº”ä¼˜åŒ–å™¨
        "adam": optim.Adam(model_params, lr=lr, weight_decay=weight_decay),
        # âš™ï¸ Transformer & SOTAæ¨¡å‹å¸¸ç”¨
        "adamw": optim.AdamW(model_params, lr=lr, weight_decay=weight_decay),
        # ğŸŒŠ RNN / LSTM å¸¸ç”¨
        "rmsprop": optim.RMSprop(model_params, lr=lr, alpha=0.99, weight_decay=weight_decay),
        # ğŸ“š ç¨€ç–æ•°æ®å‹å¥½
        "adagrad": optim.Adagrad(model_params, lr=lr, weight_decay=weight_decay),
        # # ğŸ§© Adamå˜ä½“ï¼Œé€‚åˆå¤§æ¨¡å‹
        "adamax": optim.Adamax(model_params, lr=lr, weight_decay=weight_decay),
        #   # ğŸš€ Adam + Nesterov åŠ¨é‡
        "nadam": optim.NAdam(model_params, lr=lr, weight_decay=weight_decay),
    }
    optimizer = opt_map[optimizer_name]

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    sched_map = {
        # â³ æ¯éš” step_size é™ lr
        "steplr": optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),
        # ğŸªœ åœ¨æŒ‡å®š epoch ç‚¹è¡°å‡ lr
        "multistep": optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1),
        # ğŸ“‰ æŒ‡æ•°è¡°å‡
        "exp": optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),
        #  ğŸŒŠ ä½™å¼¦é€€ç«ï¼Œå¸¸ç”¨äºåˆ†ç±»
        "cosine": optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs),
        # ğŸ”¥ğŸŒŠ ä½™å¼¦ + çƒ­é‡å¯
        "cosine_warmup": optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2),
        # ğŸ›‘ éªŒè¯é›†åœæ»æ—¶è‡ªåŠ¨é™ä½ lr
        "reduce_on_plateau": optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience),
    }
    scheduler = sched_map[scheduler_name] if scheduler_name else None

    return loss_fn, optimizer, scheduler





