import numpy as np
import torch.nn.utils
from matplotlib import pyplot as plt
from sklearn.metrics import f1_score, classification_report
from torch import nn, optim
from tqdm import tqdm


def train_epoch(model, train_loader, criterion, optimizer, device, grad_clip_value=None):
    """
    å¯¹æ¨¡å‹æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰ã€‚

    Args:
        model (torch.nn.Module): è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_loader (torch.utils.data.DataLoader): åŒ…å«è®­ç»ƒæ•°æ®çš„ DataLoaderï¼Œæ¯ä¸ªæ‰¹æ¬¡è¿”å› (images, labels)ã€‚
        criterion (torch.nn.Module): æŸå¤±å‡½æ•°ã€‚
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨ã€‚
        device (torch.device or str): æ¨¡å‹å’Œæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚
        grad_clip_value (float, optional): æ¢¯åº¦å‰ªè£é˜ˆå€¼ï¼Œè¶…è¿‡è¯¥å€¼çš„æ¢¯åº¦å°†è¢«è£å‰ªã€‚é»˜è®¤ None è¡¨ç¤ºä¸è£å‰ªã€‚

    Returns:
        avg_loss (float): è¯¥è½®è®­ç»ƒçš„å¹³å‡æŸå¤±ã€‚
        accuracy (float): è¯¥è½®è®­ç»ƒçš„æ•´ä½“å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€‚
        f1 (float): è¯¥è½®è®­ç»ƒçš„å®å¹³å‡ F1 åˆ†æ•°ã€‚
        all_preds (np.ndarray): æœ¬è½®è®­ç»ƒæ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ã€‚
        all_labels (np.ndarray): æœ¬è½®è®­ç»ƒæ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾ã€‚
    """

    model.train()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(train_loader, desc='Training')
    for batch_idx, (images, labels) in enumerate(progress_bar):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()                           # æ¸…ç©ºä¸Šæ¬¡è¿­ä»£çš„æ¢¯åº¦
        outputs = model(images)                         # å‰å‘ä¼ æ’­ï¼Œè·å¾—é¢„æµ‹å€¼
        loss = criterion(outputs, labels)               # è®¡ç®—æŸå¤±

        loss.backward()                                 # åå‘ä¼ æ’­

        if grad_clip_value is not None:                 # æ¢¯åº¦å‰ªè£
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)

        optimizer.step()                                # æ›´æ–°æ¨¡å‹å‚æ•°

        total_loss += loss.item()                       # è®¡ç®—æ€»æŸå¤±

        _, predicted = torch.max(outputs.data, 1)       # è·å–é¢„æµ‹ç±»åˆ«
        total += labels.size(0)
        correct += (predicted == labels).sum().item()   # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®æ•°é‡

        all_preds.extend(predicted.cpu().numpy())       # æ”¶é›†å•æ‰¹æ¬¡æ‰€æœ‰çš„é¢„æµ‹ç±»åˆ«
        all_labels.extend(labels.cpu().numpy())         # æ”¶é›†å•æ‰¹æ¬¡æ‰€æœ‰çš„çœŸå®ç±»åˆ«

        progress_bar.set_postfix({'Loss': loss.item(),
                                  'Acc': 100. * correct / total})

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    f1 = f1_score(all_labels, all_preds,
                  average='macro', zero_division=0)

    return avg_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)


def validate_epoch(model, val_loader, criterion, device):
    """
    å¯¹æ¨¡å‹æ‰§è¡Œä¸€ä¸ªéªŒè¯è½®æ¬¡ï¼ˆepochï¼‰ï¼Œè®¡ç®—æŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°ã€‚

    Args:
        model (torch.nn.Module): è¦éªŒè¯çš„æ¨¡å‹ã€‚
        val_loader (torch.utils.data.DataLoader): éªŒè¯æ•°æ®çš„ DataLoaderï¼Œæ¯ä¸ªæ‰¹æ¬¡è¿”å› (images, labels)ã€‚
        criterion (torch.nn.Module): æŸå¤±å‡½æ•°ã€‚
        device (torch.device or str): æ¨¡å‹å’Œæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚

    Returns:
        avg_loss (float): è¯¥è½®éªŒè¯çš„å¹³å‡æŸå¤±ã€‚
        accuracy (float): è¯¥è½®éªŒè¯çš„æ•´ä½“å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€‚
        f1 (float): è¯¥è½®éªŒè¯çš„å®å¹³å‡ F1 åˆ†æ•°ã€‚
        all_preds (np.ndarray): æœ¬è½®éªŒè¯æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ã€‚
        all_labels (np.ndarray): æœ¬è½®éªŒè¯æ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾ã€‚
    """

    model.eval()                                        # è¯„ä¼°æ¨¡å¼ï¼Œå›ºå®šæ¨¡å‹å‚æ•°ï¼Œç¦æ­¢åå‘ä¼ æ’­
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():                               # withoutæ¢¯åº¦ä¿¡æ¯
        for images, labels in tqdm(val_loader, desc="Validating"):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(val_loader)             # å¹³å‡æŸå¤±ï¼šæ€»æŸå¤± / batchæ•°é‡
    accuracy = 100. * correct / total                   # å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•° * 100%
    f1 = f1_score(all_labels, all_preds,                # ä½¿ç”¨æ‰€æœ‰é¢„æµ‹ç±»åˆ«å’ŒçœŸå®ç±»åˆ«ï¼Œè°ƒç”¨sklearn çš„å‡½æ•°è®¡ç®— F1
                  average='macro', zero_division=0)

    return avg_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)


def train_model(model, train_loader, val_loader,
                num_epochs=50, lr=1e-4,weight_decay=0.01,
                loss_name="crossentropy", optimizer_name="adamw", scheduler_name="cosine",
                device='cuda', patience=10, min_delta=1e-4,
                grad_clip_value=None, display_interval=10, model_save_path='best_model'):
    """
    è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æ¯è½®éªŒè¯åè¯„ä¼°æ€§èƒ½ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œæ”¯æŒæ—©åœå’ŒæŒ‡æ ‡å¯è§†åŒ–ã€‚

    Args:
        model (torch.nn.Module): è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_loader (DataLoader): è®­ç»ƒæ•°æ® DataLoaderï¼Œæ¯æ‰¹è¿”å› (images, labels)ã€‚
        val_loader (DataLoader): éªŒè¯æ•°æ® DataLoaderï¼Œæ¯æ‰¹è¿”å› (images, labels)ã€‚
        num_epochs (int, optional): æœ€å¤§è®­ç»ƒè½®æ•°ã€‚é»˜è®¤ 50ã€‚
        lr (float, optional): å­¦ä¹ ç‡ã€‚é»˜è®¤ 1e-4ã€‚
        weight_decay (float, optional): æƒé‡è¡°å‡ç³»æ•°ã€‚é»˜è®¤ 0.01ã€‚
        loss_name (str, optional): æŸå¤±å‡½æ•°åç§°ã€‚é»˜è®¤ "crossentropy"ã€‚
        optimizer_name (str, optional): ä¼˜åŒ–å™¨åç§°ã€‚é»˜è®¤ "adamw"ã€‚
        scheduler_name (str, optional): å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°ã€‚é»˜è®¤ "cosine"ã€‚
        device (str or torch.device, optional): æ¨¡å‹å’Œæ•°æ®æ‰€åœ¨è®¾å¤‡ã€‚é»˜è®¤ "cuda"ã€‚
        patience (int, optional): æ—©åœè½®æ•°é˜ˆå€¼ã€‚é»˜è®¤ 10ã€‚
        min_delta (float, optional): éªŒè¯ç²¾åº¦æå‡é˜ˆå€¼ï¼Œç”¨äºæ—©åœåˆ¤å®šã€‚é»˜è®¤ 1e-4ã€‚
        grad_clip_value (float, optional): æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ŒNone è¡¨ç¤ºä¸è£å‰ªã€‚é»˜è®¤ Noneã€‚
        display_interval (int, optional): æ‰“å°å¹³å‡æŒ‡æ ‡çš„é—´éš”è½®æ•°ã€‚é»˜è®¤ 10ã€‚
        model_save_path (str, optional): ä¿å­˜æœ€ä½³æ¨¡å‹çš„è·¯å¾„ã€‚é»˜è®¤ 'best_model'ã€‚

    Returns:
        best_model_state (dict): æœ€ä½³æ¨¡å‹çš„ state_dictã€‚
        metrics (dict): è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
            - train_losses, val_losses, train_accuracies, val_accuracies
            - train_f1s, val_f1s
            - best_val_accuracy
    """

    criterion, optimizer, scheduler = (                 # é…ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨
        _build_training_config(
        model.parameters(), loss_name=loss_name,
        optimizer_name=optimizer_name,
        scheduler_name=scheduler_name,
        lr=lr, weight_decay=weight_decay,
        num_epochs=num_epochs, patience=patience
    ))

    class_names = []

    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    train_f1s = []
    val_f1s = []

    best_val_acc = 0
    best_model_state = None
    patience_counter = 0

    epoch_group_metrics = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'train_f1': [], 'val_f1': []
    }

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 50)

        train_loss, train_acc, train_f1, train_preds, train_labels = train_epoch(
            model, train_loader, criterion, optimizer, device, grad_clip_value
        )

        val_loss, val_acc, val_f1, val_preds, val_labels = validate_epoch(
            model, val_loader, criterion, device
        )

        scheduler.step()                                # æ›´æ–°å­¦ä¹ ç‡

        train_losses.append(train_loss)                 # ä¿å­˜æŒ‡æ ‡ç»“æœ
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        train_f1s.append(train_f1)
        val_f1s.append(val_f1)

        epoch_group_metrics['train_loss'].append(train_loss)
        epoch_group_metrics['val_loss'].append(val_loss)
        epoch_group_metrics['train_acc'].append(train_acc)
        epoch_group_metrics['val_acc'].append(val_acc)
        epoch_group_metrics['train_f1'].append(train_f1)
        epoch_group_metrics['val_f1'].append(val_f1)

        print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        print(f"Train Accuracy: {train_acc:.2f}%, Val Accuracy: {val_acc:.2f}%")
        print(f"Train Macro F1: {train_f1:.4f}, Val Macro F1: {val_f1:.4f}")
        print(f"Learning Rate: {scheduler.get_last_lr()[0]:.6f}")

        if (epoch + 1) % display_interval == 0:         # æ¯display_intervalä¸ªepochæ‰“å°ä¸€æ¬¡å¹³å‡†æŒ‡æ ‡
            print("\n" + "=" * 60)
            print(f"AVERAGE METRICS FOR EPOCHS {epoch - display_interval + 2}-{epoch + 1}")
            print("=" * 60)
            avg_train_loss = np.mean(epoch_group_metrics['train_loss'])
            avg_val_loss = np.mean(epoch_group_metrics['val_loss'])
            avg_train_acc = np.mean(epoch_group_metrics['train_acc'])
            avg_val_acc = np.mean(epoch_group_metrics['val_acc'])
            avg_train_f1 = np.mean(epoch_group_metrics['train_f1'])
            avg_val_f1 = np.mean(epoch_group_metrics['val_f1'])

            print(f"Average Train Loss: {avg_train_loss:.4f} | Average Val Loss: {avg_val_loss:.4f}")
            print(f"Average Train Accuracy: {avg_train_acc:.2f}% | Average Val Accuracy: {avg_val_acc:.2f}%")
            print(f"Average Train Macro F1: {avg_train_f1:.4f} | Average Val Macro F1: {avg_val_f1:.4f}")
            print("=" * 60)

            for key in epoch_group_metrics:             # æ¸…ç©ºç´¯ç§¯æŒ‡æ ‡
                epoch_group_metrics[key] = []

        if val_acc > best_val_acc + min_delta:          # ä¿å­˜æœ€å¥½çš„æ¨¡å‹æƒé‡
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0

            torch.save({
                'epoch': epoch,
                'model_state_dict': best_model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_accuracy': best_val_acc,
                'val_f1': val_f1,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_accuracies': train_accuracies,
                'val_accuracies': val_accuracies,
                'train_f1s': train_f1s,
                'val_f1s': val_f1s
            }, model_save_path)
            print(f"ğŸ‰ New best model saved with accuracy: {best_val_acc:.2f}%")
        else:
            patience_counter += 1
            print(f"â³ No improvement for {patience_counter} epochs")

        if patience_counter >= patience:                # æ—©åœæœºåˆ¶
            print(f"\nğŸ›‘ Early stopping triggered after {patience} epochs without improvement")
            break

    print("\n" + "=" * 60)
    print("TRAINING COMPLETED!")
    print("=" * 60)
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
    print(f"Total Epochs: {epoch + 1}")

    _plot_training_results(train_losses, val_losses,  # ç»˜åˆ¶å›¾åƒ
                           train_accuracies, val_accuracies,
                           train_f1s, val_f1s)


    if best_model_state is not None:                    # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆéªŒè¯
        model.load_state_dict(best_model_state)
        model.eval()

        final_val_loss, final_val_acc, final_val_f1, final_val_preds, final_val_labels = validate_epoch(
            model, val_loader, criterion, device
        )

        print("\nFinal Classification Report:")
        print("=" * 50)
        print(classification_report(final_val_labels, final_val_preds,
                                    target_names=class_names, digits=4))

    return best_model_state, {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        'train_f1s': train_f1s,
        'val_f1s': val_f1s,
        'best_val_accuracy': best_val_acc
    }



def _build_training_config(model_params, loss_name="crossentropy", optimizer_name="adamw",
                           scheduler_name=None, lr=1e-5, weight_decay=1e-4, num_epochs=50, patience=5):
    """
    æ„å»ºè®­ç»ƒé…ç½®ï¼šæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    Args:
        model_params: æ¨¡å‹å‚æ•°ï¼Œä¾›ä¼˜åŒ–å™¨ä½¿ç”¨ã€‚
        loss_name: æŸå¤±å‡½æ•°ç±»å‹
        optimizer_name: ä¼˜åŒ–å™¨ç±»å‹
        scheduler_name: å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
        lr (float, optional): å­¦ä¹ ç‡ï¼Œé»˜è®¤ 1e-5ã€‚
        weight_decay (float, optional): æƒé‡è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ 1e-4ã€‚
        num_epochs (int, optional): æ€»è®­ç»ƒè½®æ•°ï¼Œç”¨äºéƒ¨åˆ†è°ƒåº¦å™¨ï¼Œé»˜è®¤ 50ã€‚
        patience (int, optional): éªŒè¯é›†åœæ»æ—¶é™ä½å­¦ä¹ ç‡çš„ç­‰å¾…è½®æ•°ï¼Œä»…å¯¹ ReduceLROnPlateau æœ‰æ•ˆï¼Œé»˜è®¤ 5ã€‚

    Returns:
        tuple:
            loss_fn (torch.nn.Module): é€‰å®šçš„æŸå¤±å‡½æ•°å®ä¾‹ã€‚
            optimizer (torch.optim.Optimizer): é€‰å®šçš„ä¼˜åŒ–å™¨å®ä¾‹ã€‚
            scheduler (torch.optim.lr_scheduler._LRScheduler or None): é€‰å®šçš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹ï¼Œè‹¥ scheduler_name ä¸º None åˆ™è¿”å› Noneã€‚
    """

    # æŸå¤±å‡½æ•°
    loss_map = {
        # ğŸ¯ æ ‡é…å¤šåˆ†ç±»æŸå¤±
        "crossentropy": nn.CrossEntropyLoss(),
        # ğŸ“‰ è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆlog_softmax æ­é…ï¼‰
        "nll": nn.NLLLoss(),
        # ğŸ§Š æ ‡ç­¾å¹³æ»‘ï¼Œé˜²è¿‡æ‹Ÿåˆ
        "label_smoothing": nn.CrossEntropyLoss(label_smoothing=0.1),
        # âœ… äºŒåˆ†ç±»ç¨³å®šé¦–é€‰
        "bce": nn.BCEWithLogitsLoss(),
    }
    loss_fn = loss_map[loss_name]

    # ä¼˜åŒ–å™¨
    opt_map = {
        # ğŸƒâ€â™‚ï¸ åŸºç¡€ç‰ˆï¼Œæ…¢ä½†ç¨³
        "sgd": optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay),
        # ğŸ¹ SGDåŠ é€Ÿç‰ˆ
        "sgd_nesterov": optim.SGD(model_params, lr=lr, momentum=0.9, nesterov=True, weight_decay=weight_decay),
        # âš¡ å¸¸è§è‡ªé€‚åº”ä¼˜åŒ–å™¨
        "adam": optim.Adam(model_params, lr=lr, weight_decay=weight_decay),
        # âš™ï¸ Transformer & SOTAæ¨¡å‹å¸¸ç”¨
        "adamw": optim.AdamW(model_params, lr=lr, weight_decay=weight_decay),
        # ğŸŒŠ RNN / LSTM å¸¸ç”¨
        "rmsprop": optim.RMSprop(model_params, lr=lr, alpha=0.99, weight_decay=weight_decay),
        # ğŸ“š ç¨€ç–æ•°æ®å‹å¥½
        "adagrad": optim.Adagrad(model_params, lr=lr, weight_decay=weight_decay),
        # # ğŸ§© Adamå˜ä½“ï¼Œé€‚åˆå¤§æ¨¡å‹
        "adamax": optim.Adamax(model_params, lr=lr, weight_decay=weight_decay),
        #   # ğŸš€ Adam + Nesterov åŠ¨é‡
        "nadam": optim.NAdam(model_params, lr=lr, weight_decay=weight_decay),
    }
    optimizer = opt_map[optimizer_name]

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    sched_map = {
        # â³ æ¯éš” step_size é™ lr
        "steplr": optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),
        # ğŸªœ åœ¨æŒ‡å®š epoch ç‚¹è¡°å‡ lr
        "multistep": optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1),
        # ğŸ“‰ æŒ‡æ•°è¡°å‡
        "exp": optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),
        #  ğŸŒŠ ä½™å¼¦é€€ç«ï¼Œå¸¸ç”¨äºåˆ†ç±»
        "cosine": optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs),
        # ğŸ”¥ğŸŒŠ ä½™å¼¦ + çƒ­é‡å¯
        "cosine_warmup": optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2),
        # ğŸ›‘ éªŒè¯é›†åœæ»æ—¶è‡ªåŠ¨é™ä½ lr
        "reduce_on_plateau": optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience),
    }
    scheduler = sched_map[scheduler_name] if scheduler_name else None

    return loss_fn, optimizer, scheduler


def _plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies,
                           train_f1s, val_f1s, save_path='training_results.png'):
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°æ›²çº¿ï¼Œå¹¶ä¿å­˜å›¾åƒã€‚

    Args:
        train_losses (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„å¹³å‡è®­ç»ƒæŸå¤±ã€‚
        val_losses (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„å¹³å‡éªŒè¯æŸå¤±ã€‚
        train_accuracies (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„è®­ç»ƒé›†å‡†ç¡®ç‡ (%ï¼‰ã€‚
        val_accuracies (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„éªŒè¯é›†å‡†ç¡®ç‡ (%ï¼‰ã€‚
        train_f1s (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„è®­ç»ƒé›†å®å¹³å‡ F1 åˆ†æ•°ã€‚
        val_f1s (list of float): æ¯ä¸ªè®­ç»ƒ epoch çš„éªŒè¯é›†å®å¹³å‡ F1 åˆ†æ•°ã€‚
        save_path (str, optional): ä¿å­˜å›¾åƒçš„æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ 'training_results.png'ã€‚
    """

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(train_losses, label='Train Loss', color='blue')
    axes[0, 0].plot(val_losses, label='Val Loss', color='red')
    axes[0, 0].set_title('Training and Validation Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(train_accuracies, label='Train Accuracy', color='blue')
    axes[0, 1].plot(val_accuracies, label='Val Accuracy', color='red')
    axes[0, 1].set_title('Training and Validation Accuracy')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # F1åˆ†æ•°æ›²çº¿
    axes[1, 0].plot(train_f1s, label='Train Macro F1', color='blue')
    axes[1, 0].plot(val_f1s, label='Val Macro F1', color='red')
    axes[1, 0].set_title('Training and Validation Macro F1')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Macro F1 Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # æœ€ååä¸ªepochçš„è¯¦ç»†æŒ‡æ ‡
    recent_epochs = min(10, len(train_losses))
    epochs_range = range(len(train_losses) - recent_epochs, len(train_losses))

    axes[1, 1].plot(epochs_range, train_accuracies[-recent_epochs:], 'o-', label='Train Acc', color='blue')
    axes[1, 1].plot(epochs_range, val_accuracies[-recent_epochs:], 'o-', label='Val Acc', color='red')
    axes[1, 1].set_title('Last 10 Epochs - Accuracy')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy (%)')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()